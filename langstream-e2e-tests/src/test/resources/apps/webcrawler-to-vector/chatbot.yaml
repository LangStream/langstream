#
# Copyright DataStax, Inc.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
# http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

topics:
  - name: "ls-test-questions-topic"
    creation-mode: create-if-not-exists
  - name: "ls-test-answers-topic"
    creation-mode: create-if-not-exists
  - name: "ls-test-log-topic"
    creation-mode: create-if-not-exists
errors:
    on-failure: "skip"
resources:
  size: 2
pipeline:
  - name: "convert-to-structure"
    type: "document-to-json"
    input: "ls-test-questions-topic"
    configuration:
      text-field: "question"
  - name: "compute-embeddings"
    type: "compute-ai-embeddings"
    configuration:
      ai-service: "${secrets.embeddings.service}"
      model: "${secrets.embeddings.model}"
      embeddings-field: "value.question_embeddings"
      text: "{{% value.question }}"
      flush-interval: 0
  - name: "lookup-related-documents-in-llm"
    type: "query"
    configuration:
      datasource: "AstraDatasource"
      query: "SELECT text FROM langstreamtest.documents ORDER BY embeddings_vector ANN OF ? LIMIT 5"
      fields:
        - "value.question_embeddings"
      output-field: "value.related_documents"
  - name: "ai-chat-completions"
    type: "ai-chat-completions"

    configuration:
      ai-service: "${secrets.chat-completions.service}"
      model: "${secrets.chat-completions.model}" # This needs to be set to the model deployment name, not the base name
      # on the ls-test-log-topic we add a field with the answer
      completion-field: "value.answer"
      # we are also logging the prompt we sent to the LLM
      log-field: "value.prompt"
      # here we configure the streaming behavior
      # as soon as the LLM answers with a chunk we send it to the ls-test-answers-topic
      stream-to-topic: "ls-test-answers-topic"
      # on the streaming answer we send the answer as whole message
      # the 'value' syntax is used to refer to the whole value of the message
      stream-response-completion-field: "value"
      # we want to stream the answer as soon as we have 20 chunks
      # in order to reduce latency for the first message the agent sends the first message
      # with 1 chunk, then with 2 chunks....up to the min-chunks-per-message value
      # eventually we want to send bigger messages to reduce the overhead of each message on the topic
      min-chunks-per-message: 20
      messages:
        - role: system
          content: |
              An user is going to perform a questions, he documents below may help you in answering to their questions.
              Please try to leverage them in your answer as much as possible.
              Take into consideration that the user is always asking questions about the LangStream project.
              If you provide code or YAML snippets, please explicitly state that they are examples.
              Do not provide information that is not related to the LangStream project.
            
              Documents:
              {{# value.related_documents}}
              {{% text}}
              {{%/ value.related_documents}}
        - role: user
          content: "{{% value.question}}"
  - name: "cleanup-response"
    type: "drop-fields"
    output: "ls-test-log-topic"
    configuration:
      fields:
        - "question_embeddings"
        - "related_documents"